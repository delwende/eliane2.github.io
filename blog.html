<!DOCTYPE HTML>

<html>
	<head>
		<title>Eliane Birba</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/jquery.dropotron.min.js"></script>
		<script src="js/jquery.scrolly.min.js"></script>
		<script src="js/jquery.scrollgress.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-wide.css" />
			<link rel="stylesheet" href="css/style-noscript.css" />
		</noscript>
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="css/ie/v9.css" /><![endif]-->
	</head>
	<body class="no-sidebar">

		<!-- Header -->
			<header id="header" class="skel-layers-fixed">
				<h1 id="logo"><a href="index.html">Eliane <span>Birba</span></a></h1>
				<nav id="nav">
					<ul>
						<li class="current"><a href="index.html">Home</a></li>
						<li><a href="mailto:birbadelwende@gmail.com" class="button special"><span class="fa fa-envelope-o"></span> Contact</a></li>
					</ul>
				</nav>
			</header>

		<!-- Main -->
			<article id="main">
				<!--
				<header class="special container">
					<span class="icon fa-mobile"></span>
					<h2>And finally there's <strong>No Sidebar</strong></h2>
					<p>Where that in the center faces the nameless horrors alone.</p>
				</header>
				 -->

				<!-- One -->
					<section class="wrapper style4 container">

						<!-- Content -->
							<!--<div class="content">
								<section>-->
							<header>
								<h3><strong>"Next comming course: Scalable Machine learning and deep learning"</strong></h3>
							</header>
							
							<p>This course marries data parallel programming with deep learning, and helps students to work on distributed deep learning problems with big datasets. At the end of the course students will be familiar with the main machine learning and deep learning algorithms and know how to implement them using data parallel programming platforms, such as Spark and TensorFlow, on a cluster of computers and apply them on massive datasets. This course has a system-based focus, that is, student will learn not only the theory of machine learning and deep learning, but also the practical aspects of building large scale systems that take advantage of machine learning and deep learning. The course is divided into two parts. The focus of the first part is to introduce the data parallel platforms for machine learning and deep learning, and the goal of the second part is to present the deep learning algorithms and teach students how to implement them using the introduced platforms in the first part. </p>
							<div class="4u 12u(narrower) important(narrower)">
							
								<img src="images/plan.png" class = "round">

							</div>
							<!--
							<header>
								<h3><strong>"At the intersection of Reinforcement Learning and Deep Learning"</strong></h3>
							</header>
							<p>
							The recent successes of Deep Learning have opened up the way to a wide range of research that combine both fields.

							<br><br>
							Such an application is simply to use deep neural networks as function approximators used in RL algorithms.
							An example of such success is DeepMind's 
							<a href="http://home.uchicago.edu/~arij/journalclub/papers/2015_Mnih_et_al.pdf"> deep Q network</a> , which is able to learn how to play a set of different video games at a human level performance (or better) without being provided any game-specific information.
							<br><br>
							On the other side of the spectrum,
							policy gradient methods (such as the REINFORCE rule) from the Reinforcement Learning literature have enabled the introduction of non-differentiable steps in deep neural network architectures.
							Such models, which are referred to <strong>hard attention models</strong> because the network is equipped with sensors/attention that it learns to deploy, have shown to be very promising for future work. In particular, they are scalable since they do not couple representation power with computation time (as opposed to vanilla neural networks) and have a better generalization power. See for example
							<a href="http://arxiv.org/abs/1406.6247">Recurrent Models of Visual Attention</a> and
							<a href="http://arxiv.org/abs/1505.00521">Reinforcement Learning Neural Turing Machines</a>..</p>
							<p>	Interested in more complex simulators
							My current main research project is to identify
							AI lab stanford </p>
								<!--</section>
							</div>-->
					</section>
			</article>

		<!-- Footer -->
			<footer id="footer">

				<ul>
				Find me on 
				<a href=" target="blank"> Twitter </a> /
				<a href="https://www.facebook.com//eliane.birba" target="_blank">Facebook</a> / 
				<a href="https://www.linkedin.com/in/birba-eliane-8919a389/" target="_blank">LinkedIn</a> /
				
				
				</ul>	

				<ul class="copyright">
					<li>&copy; 2019. All rights reserved</li>
				</ul>

			</footer>

	</body>
</html>